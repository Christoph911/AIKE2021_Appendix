{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Retriever_Evaluation.ipynb","provenance":[{"file_id":"https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial5_Evaluation.ipynb","timestamp":1628511214392}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"cells":[{"cell_type":"markdown","metadata":{"id":"h-LwY9E3DI-I"},"source":["# **Retrieval and QA Evaluation on German Legal Data**\n","\n","This Notebook contains the evaluation of different Information Retrieval and QA methods on German legal documents. \n","\n","We compare BM25 and Dense Passge Retrieval (DPR) for Document/Passage Retrieval purposes and take first steps in the evaluation of an BERT-based QA model. "]},{"cell_type":"markdown","metadata":{"id":"SgIq-8fSChao"},"source":["## 1. Init Environment\n","\n","- Install latest release from Haystack \n","- Install ElasticSearch\n"]},{"cell_type":"code","metadata":{"id":"vgmFOp82Oht_"},"source":["!pip install grpcio-tools==1.34.1\n","!pip install git+https://github.com/deepset-ai/haystack.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNoaWcDKOhuL"},"source":["# Start Elasticsearch from source\n","! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n","! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n","! chown -R daemon:daemon elasticsearch-7.9.2\n","\n","import os\n","from subprocess import Popen, PIPE, STDOUT\n","es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],\n","                   stdout=PIPE, stderr=STDOUT,\n","                   preexec_fn=lambda: os.setuid(1)  # as daemon\n","                  )\n","# wait until ES has started\n","! sleep 30"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w0MHgxrYOhur"},"source":["# check if cuda is working\n","from farm.utils import initialize_device_settings\n","\n","device, n_gpu = initialize_device_settings(use_cuda=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e3OZKMiiCv7Y"},"source":["## 2. Create ElasticSearch DocumentStore\n","\n","- Init ElasticSearch\n","- Init data Preprocessor\n","- Write documents to ElasticSearch (optional: Preprocess documents)"]},{"cell_type":"code","metadata":{"id":"-ansYnQ43a9e"},"source":["# create indices \n","doc_index = \"evaluation_docs\"\n","label_index = \"evaluation_labels\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_NEtezLOhu5"},"source":["# Connect to Elasticsearch\n","from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n","\n","document_store = ElasticsearchDocumentStore(host=\"localhost\", \n","                                            username=\"\", \n","                                            password=\"\", \n","                                            index=\"document\",\n","                                            create_index=False, \n","                                            embedding_field=\"emb\",\n","                                            embedding_dim=768, \n","                                            excluded_meta_data=[\"emb\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRFsQUAJOhu_"},"source":["from haystack.preprocessor import PreProcessor\n","\n","# Write evaluation data to Elasticsearch Document Store\n","# split documents into passages using the PreProcessor\n","\n","preprocessor = PreProcessor(\n","    split_length=100,\n","    split_overlap=0,\n","    split_respect_sentence_boundary=False,\n","    clean_empty_lines=False,\n","    clean_whitespace=False\n",")\n","# make sure to delete documents before writing evaluation data!\n","\n","document_store.delete_documents(index=doc_index)\n","document_store.delete_documents(index=label_index)\n","\n","# add evaluation data \n","document_store.add_eval_data(\n","    filename=\"FILENAME\",\n","    doc_index=doc_index,\n","    label_index=label_index,\n","    preprocessor=preprocessor\n",")\n","\n","# Create needed label format for retriever and the reader evaluation\n","labels = document_store.get_all_labels_aggregated(index=label_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gmk6d3a-LY5m"},"source":["# check number of documents in DocumentStore\n","document_store.get_document_count(index=doc_index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gy8YwmSYOhvE"},"source":["## Initialize Retriever\n","\n","- Init BM25\n","- Init DPR"]},{"cell_type":"code","metadata":{"id":"JkhaPMIJOhvF"},"source":["# Init BM25 Retriever\n","from haystack.retriever.sparse import ElasticsearchRetriever\n","retriever = ElasticsearchRetriever(document_store=document_store)\n","\n","#Init DensePassageRetriever\n","from haystack.retriever.dense import DensePassageRetriever\n","retriever_dense = DensePassageRetriever(document_store=document_store,\n","                                 query_embedding_model=\"deepset/gbert-base-germandpr-question_encoder\",\n","                                 passage_embedding_model=\"deepset/gbert-base-germandpr-ctx_encoder\",\n","                                 use_gpu=True,\n","                                 embed_title=True,\n","                                 batch_size=16)\n","\n","# Update document embeddings in database \n","document_store.update_embeddings(retriever_dense, index=doc_index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwkBgzh5OhvR"},"source":["## Evaluation of BM25\n","\n","- Evaluate BM25 on documents with different k-value"]},{"cell_type":"code","metadata":{"id":"YzvLhnx3OhvS"},"source":["for k in range (10,110,10):\n","\n","  retriever_eval_results = retriever.eval(top_k=k, label_index=label_index, doc_index=doc_index)\n","  ## Retriever Recall is the proportion of questions for which the correct document containing the answer is among the correct documents\n","  print(\"Retriever Recall:\", retriever_eval_results[\"recall\"])\n","  ## Retriever Mean Avg Precision rewards retrievers that give relevant documents a higher rank\n","  print(\"Retriever Mean Avg Precision:\", retriever_eval_results[\"map\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nG7aE8X9HbZT"},"source":["## Evaluation of DPR\n","\n","- Evaluation of DPR on documents with different k-value"]},{"cell_type":"code","metadata":{"id":"aOler7PpHTyD"},"source":["for k in range (10,110,10):\n","\n","  ## Evaluate Retriever on its own\n","  retriever_eval_results = retriever_dense.eval(top_k=k, label_index=label_index, doc_index=doc_index)\n","  ## Retriever Recall is the proportion of questions for which the correct document containing the answer is among the correct documents\n","  print(\"Retriever Recall:\", retriever_eval_results[\"recall\"])\n","  ## Retriever Mean Avg Precision rewards retrievers that give relevant documents a higher rank\n","  print(\"Retriever Mean Avg Precision:\", retriever_eval_results[\"map\"])"],"execution_count":null,"outputs":[]}]}